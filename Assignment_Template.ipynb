{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin Number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Overview (provide your video link here too)\n",
    "\n",
    "This project predicts whether a student will complete an online course based on engagement, demographic, and course-related features from the `Course_Completion_Prediction.csv` dataset (100,000 records, 40 features). The target variable is **Completed** (binary: Completed / Not Completed). We apply a full data science workflow: EDA, preprocessing, feature engineering, model training, comparison, hyperparameter tuning, and cross-validation.\n",
    "\n",
    "**Video link:** *(insert your video link here)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='table_of_contents'></a>\n",
    "\n",
    "1. [Import libraries](#imports)\n",
    "2. [Import data](#import_data)\n",
    "3. [Data exploration](#data_exploration)\n",
    "4. [Data cleaning and preparation](#data_cleaning)\n",
    "5. [Model training](#model_training)<br>\n",
    "6. [Model comparsion](#model_comparsion)<br>\n",
    "7. [Tuning](#tuning)<br>\n",
    "8. [Validation](#validation)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries <a id='imports'></a>\n",
    "[Back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import data <a id='import_data'></a>\n",
    "[Back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Course_Completion_Prediction.csv')\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data exploration <a id='data_exploration'></a>\n",
    "[Back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset-Specific Constraint: Class Imbalance\n",
    "\n",
    "A key constraint of this dataset is **class imbalance** in the target variable `Completed`. As we will see below, one class significantly outnumbers the other. This imbalance can bias models toward the majority class, leading to misleadingly high accuracy but poor recall for the minority class. This constraint influences our choices throughout:\n",
    "- **EDA**: We explicitly check and visualize the class distribution.\n",
    "- **Model Selection**: We prefer models and metrics (Precision, Recall, F1) that handle imbalance well, and use `class_weight='balanced'` where supported.\n",
    "- **Conclusion**: We assess whether the imbalance was adequately addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target variable distribution\n",
    "print('Target variable distribution:')\n",
    "print(df['Completed'].value_counts())\n",
    "print(f'\\nPercentage:')\n",
    "print(df['Completed'].value_counts(normalize=True).round(4) * 100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "df['Completed'].value_counts().plot(kind='bar', color=['#2ecc71', '#e74c3c'], ax=ax)\n",
    "ax.set_title('Target Variable Distribution (Completed)')\n",
    "ax.set_xlabel('Completion Status')\n",
    "ax.set_ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print('Missing values per column:')\n",
    "print(df.isnull().sum())\n",
    "print(f'\\nTotal missing values: {df.isnull().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "print(f'Duplicate rows: {df.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of numerical features\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f'Numerical columns ({len(numerical_cols)}): {numerical_cols}')\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(18, 14))\n",
    "axes = axes.flatten()\n",
    "for i, col in enumerate(numerical_cols[:16]):\n",
    "    axes[i].hist(df[col], bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[i].set_title(col, fontsize=10)\n",
    "plt.suptitle('Distribution of Numerical Features', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of categorical features\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "cat_plot_cols = [c for c in categorical_cols if c not in ['Student_ID', 'Name', 'Enrollment_Date', 'Course_ID', 'Course_Name']]\n",
    "print(f'Categorical columns for plotting: {cat_plot_cols}')\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "for i, col in enumerate(cat_plot_cols[:9]):\n",
    "    df[col].value_counts().plot(kind='bar', ax=axes[i], color='steelblue')\n",
    "    axes[i].set_title(col, fontsize=10)\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "plt.suptitle('Distribution of Categorical Features', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap of numerical features\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "corr = df[numerical_cols].corr()\n",
    "sns.heatmap(corr, annot=False, cmap='coolwarm', center=0, ax=ax)\n",
    "ax.set_title('Correlation Heatmap of Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots of key features vs target\n",
    "key_features = ['Progress_Percentage', 'Video_Completion_Rate', 'Quiz_Score_Avg', 'Time_Spent_Hours']\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "for i, col in enumerate(key_features):\n",
    "    sns.boxplot(x='Completed', y=col, data=df, ax=axes[i])\n",
    "    axes[i].set_title(f'{col} vs Completed')\n",
    "plt.suptitle('Key Feature Distributions by Completion Status', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA Findings\n",
    "\n",
    "1. **Class Imbalance (Dataset Constraint):** The target variable shows an imbalanced distribution \u2014 more students completed their courses than those who did not. This must be addressed during modelling to avoid biased predictions.\n",
    "2. **Key Differentiating Features:** `Progress_Percentage`, `Video_Completion_Rate`, `Quiz_Score_Avg`, and `Days_Since_Last_Login` show clear separation between completed and not-completed students.\n",
    "3. **Correlation Structure:** Most numerical features have low mutual correlation, reducing multicollinearity concerns. However, `Progress_Percentage` and `Video_Completion_Rate` are moderately correlated.\n",
    "4. **No Missing Values in Raw Data:** The dataset appears pre-cleaned. Per the assignment requirement, we will introduce and then handle missing values in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data cleaning and preparation <a id='data_cleaning'></a>\n",
    "[Back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Point 1: Handling Missing Values \u2014 Mean Imputation vs. Median Imputation\n",
    "\n",
    "Since the dataset is pre-cleaned (no missing values), we introduce missing values in `Time_Spent_Hours` and `Quiz_Score_Avg` (5% each) for learning purposes.\n",
    "\n",
    "- **Alternative considered:** Mean imputation \u2014 simple and preserves the overall average.\n",
    "- **Chosen approach:** **Median imputation** \u2014 because EDA revealed that `Time_Spent_Hours` has a right-skewed distribution with outliers (values near 0.5 appearing frequently, possibly indicating minimum-activity records). The median is more robust to these outliers than the mean, which would be pulled upward.\n",
    "- **Dataset constraint reference:** The skewed distribution of engagement metrics is a characteristic of this dataset that makes median imputation more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce missing values for learning purposes (5% random NaN in two columns)\n",
    "np.random.seed(42)\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Introduce ~5% missing values in Time_Spent_Hours and Quiz_Score_Avg\n",
    "for col in ['Time_Spent_Hours', 'Quiz_Score_Avg']:\n",
    "    mask = np.random.rand(len(df_clean)) < 0.05\n",
    "    df_clean.loc[mask, col] = np.nan\n",
    "\n",
    "print('Missing values after introducing NaNs:')\n",
    "print(df_clean[['Time_Spent_Hours', 'Quiz_Score_Avg']].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with median (Decision Point 1: median chosen over mean due to skewed distributions)\n",
    "for col in ['Time_Spent_Hours', 'Quiz_Score_Avg']:\n",
    "    median_val = df_clean[col].median()\n",
    "    df_clean[col] = df_clean[col].fillna(median_val)\n",
    "    print(f'{col}: imputed with median = {median_val:.2f}')\n",
    "\n",
    "print(f'\\nRemaining missing values: {df_clean.isnull().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target variable\n",
    "df_clean['Completed_Binary'] = (df_clean['Completed'] == 'Completed').astype(int)\n",
    "print('Target encoding: Completed=1, Not Completed=0')\n",
    "print(df_clean['Completed_Binary'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-informative columns (IDs, names, dates, original target text)\n",
    "drop_cols = ['Student_ID', 'Name', 'Enrollment_Date', 'Course_ID', 'Course_Name', 'Completed']\n",
    "df_clean = df_clean.drop(columns=drop_cols)\n",
    "print(f'Shape after dropping non-informative columns: {df_clean.shape}')\n",
    "print(f'Remaining columns: {df_clean.columns.tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features using Label Encoding\n",
    "cat_features = df_clean.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f'Categorical features to encode: {cat_features}')\n",
    "\n",
    "le_dict = {}\n",
    "for col in cat_features:\n",
    "    le = LabelEncoder()\n",
    "    df_clean[col] = le.fit_transform(df_clean[col])\n",
    "    le_dict[col] = le\n",
    "    print(f'  {col}: {list(le.classes_)}')\n",
    "\n",
    "print(f'\\nDataset shape after encoding: {df_clean.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_clean.drop(columns=['Completed_Binary'])\n",
    "y = df_clean['Completed_Binary']\n",
    "\n",
    "print(f'Features shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')\n",
    "print(f'Target distribution:\\n{y.value_counts()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)\n",
    "X_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (80-20, stratified to preserve class distribution)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f'Training set: {X_train.shape[0]} samples')\n",
    "print(f'Test set: {X_test.shape[0]} samples')\n",
    "print(f'\\nTraining target distribution:\\n{y_train.value_counts(normalize=True).round(4)}')\n",
    "print(f'\\nTest target distribution:\\n{y_test.value_counts(normalize=True).round(4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model training <a id='model_training'></a>\n",
    "[Back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Point 2: Model Selection \u2014 Random Forest vs. Support Vector Machine (SVM)\n",
    "\n",
    "- **Alternative considered:** Support Vector Machine (SVM) \u2014 a powerful classifier for high-dimensional data.\n",
    "- **Reason SVM was not selected:** With 100,000 samples, SVM training time scales poorly (O(n\u00b2) to O(n\u00b3)), making it impractical. Additionally, SVM requires careful kernel selection and does not natively provide feature importance, which is valuable for interpreting course completion drivers.\n",
    "- **Chosen approach:** **Random Forest** \u2014 it handles large datasets efficiently, provides built-in feature importance, is robust to overfitting via ensembling, and supports `class_weight='balanced'` to address the class imbalance constraint.\n",
    "- **EDA reference:** The moderate correlations between features (seen in the heatmap) and the class imbalance both favour tree-based ensemble methods over SVM.\n",
    "\n",
    "We train four models for comparison: Logistic Regression, Decision Tree, Random Forest, and Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(class_weight='balanced', random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f'Training {name}...')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1 Score': f1_score(y_test, y_pred),\n",
    "        'model': model,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    print(f'  Accuracy: {results[name][\"Accuracy\"]:.4f}  |  F1: {results[name][\"F1 Score\"]:.4f}')\n",
    "\n",
    "print('\\nAll models trained successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model comparsion <a id='model_comparsion'></a>\n",
    "[Back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "metrics_df = pd.DataFrame({name: {k: v for k, v in vals.items() if k not in ['model', 'predictions']}\n",
    "                           for name, vals in results.items()}).T\n",
    "metrics_df = metrics_df.round(4)\n",
    "print('Model Comparison:')\n",
    "print(metrics_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "metrics_df.plot(kind='bar', ax=ax)\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend(loc='lower right')\n",
    "plt.xticks(rotation=25)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for each model\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
    "for i, (name, vals) in enumerate(results.items()):\n",
    "    cm = confusion_matrix(y_test, vals['predictions'])\n",
    "    ConfusionMatrixDisplay(cm, display_labels=['Not Completed', 'Completed']).plot(ax=axes[i], cmap='Blues')\n",
    "    axes[i].set_title(name, fontsize=10)\n",
    "plt.suptitle('Confusion Matrices', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report for the best model\n",
    "best_model_name = metrics_df['F1 Score'].idxmax()\n",
    "print(f'Best model by F1 Score: {best_model_name}')\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, results[best_model_name]['predictions'],\n",
    "                            target_names=['Not Completed', 'Completed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest\n",
    "rf_model = results['Random Forest']['model']\n",
    "importances = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "importances.head(15).plot(kind='barh', ax=ax, color='teal')\n",
    "ax.set_title('Top 15 Feature Importances (Random Forest)')\n",
    "ax.set_xlabel('Importance')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Tuning <a id='tuning'></a>\n",
    "\n",
    "[Back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform hyperparameter tuning on the Random Forest model using GridSearchCV, as it was one of the top-performing models and provides interpretable feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning using GridSearchCV on Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f'\\nBest parameters: {grid_search.best_params_}')\n",
    "print(f'Best CV F1 Score: {grid_search.best_score_:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned model on test set\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred_tuned = best_rf.predict(X_test)\n",
    "\n",
    "print('Tuned Random Forest - Test Set Performance:')\n",
    "print(f'  Accuracy:  {accuracy_score(y_test, y_pred_tuned):.4f}')\n",
    "print(f'  Precision: {precision_score(y_test, y_pred_tuned):.4f}')\n",
    "print(f'  Recall:    {recall_score(y_test, y_pred_tuned):.4f}')\n",
    "print(f'  F1 Score:  {f1_score(y_test, y_pred_tuned):.4f}')\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred_tuned, target_names=['Not Completed', 'Completed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for tuned model\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "cm_tuned = confusion_matrix(y_test, y_pred_tuned)\n",
    "ConfusionMatrixDisplay(cm_tuned, display_labels=['Not Completed', 'Completed']).plot(ax=ax, cmap='Blues')\n",
    "ax.set_title('Confusion Matrix - Tuned Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Validation <a id='validation'></a>\n",
    "\n",
    "[Back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K-Fold Cross-Validation (5-fold) on the tuned model\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_accuracy = cross_val_score(best_rf, X_scaled, y, cv=cv, scoring='accuracy')\n",
    "cv_f1 = cross_val_score(best_rf, X_scaled, y, cv=cv, scoring='f1')\n",
    "cv_precision = cross_val_score(best_rf, X_scaled, y, cv=cv, scoring='precision')\n",
    "cv_recall = cross_val_score(best_rf, X_scaled, y, cv=cv, scoring='recall')\n",
    "\n",
    "print('5-Fold Stratified Cross-Validation Results (Tuned Random Forest):')\n",
    "print(f'  Accuracy:  {cv_accuracy.mean():.4f} (+/- {cv_accuracy.std():.4f})')\n",
    "print(f'  Precision: {cv_precision.mean():.4f} (+/- {cv_precision.std():.4f})')\n",
    "print(f'  Recall:    {cv_recall.mean():.4f} (+/- {cv_recall.std():.4f})')\n",
    "print(f'  F1 Score:  {cv_f1.mean():.4f} (+/- {cv_f1.std():.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-validation results\n",
    "cv_results = pd.DataFrame({\n",
    "    'Accuracy': cv_accuracy,\n",
    "    'Precision': cv_precision,\n",
    "    'Recall': cv_recall,\n",
    "    'F1 Score': cv_f1\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "cv_results.plot(kind='box', ax=ax)\n",
    "ax.set_title('Cross-Validation Score Distribution (5-Fold)')\n",
    "ax.set_ylabel('Score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Summary of Findings\n",
    "- We successfully built a classification model to predict online course completion using the `Course_Completion_Prediction.csv` dataset (100,000 records).\n",
    "- After comparing Logistic Regression, Decision Tree, Random Forest, and Gradient Boosting classifiers, Random Forest and Gradient Boosting emerged as the top performers.\n",
    "- The tuned Random Forest model achieved strong generalisation as confirmed by 5-fold stratified cross-validation.\n",
    "\n",
    "### Dataset-Specific Constraint: Class Imbalance\n",
    "- The dataset exhibited class imbalance (more \"Completed\" than \"Not Completed\" records). This was addressed by using `class_weight='balanced'` in tree-based models and evaluating with F1 Score, Precision, and Recall rather than relying solely on accuracy.\n",
    "- Despite the imbalance, the model maintained good recall for the minority class (\"Not Completed\"), demonstrating that the mitigation strategy was effective.\n",
    "\n",
    "### Decision Points Recap\n",
    "1. **Missing value imputation:** Median imputation was chosen over mean imputation because `Time_Spent_Hours` was right-skewed with outlier-like low values, making the median more robust.\n",
    "2. **Model selection:** Random Forest was chosen over SVM because of SVM's computational cost on 100K samples, and Random Forest's ability to provide feature importance and handle class imbalance natively.\n",
    "\n",
    "### Top Predictive Features\n",
    "- `Progress_Percentage`, `Video_Completion_Rate`, `Days_Since_Last_Login`, and `Quiz_Score_Avg` were the most important features, aligning with the EDA findings.\n",
    "\n",
    "### Future Improvements\n",
    "1. Apply SMOTE (Synthetic Minority Oversampling) to further address class imbalance and potentially improve minority-class recall.\n",
    "2. Explore advanced ensemble methods (e.g., XGBoost, LightGBM) which may yield further performance gains with proper tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}